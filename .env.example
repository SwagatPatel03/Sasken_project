# Example environment configuration for 3GPP Change Detection System
# Copy this file to .env and fill in your actual values

# ===== LLM Configuration =====
# Groq API key for LLM inference (optional - system can work without it)
GROQ_API_KEY=your_groq_api_key_here

# Alternative LLM configurations (if using other providers)
# OPENAI_API_KEY=your_openai_key_here
# HUGGINGFACE_API_TOKEN=your_hf_token_here

# ===== Application Settings =====
# API server host and port
API_HOST=0.0.0.0
API_PORT=8000

# Streamlit server port
STREAMLIT_PORT=8501

# ===== Data Paths =====
# Directory for raw documents
RAW_DATA_PATH=data/raw

# Directory for processed data
PROCESSED_DATA_PATH=data/processed

# Vector database storage path
VECTOR_DB_PATH=data/embeddings

# ===== Model Settings =====
# Embedding model (from sentence-transformers)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# LLM model name (for Groq or other providers)
LLM_MODEL=llama-3.3-70b-versatile

# ===== Processing Configuration =====
# Maximum tokens per chunk
MAX_CHUNK_TOKENS=500

# Minimum tokens for chunk merging
MIN_CHUNK_TOKENS=20

# Similarity threshold for change detection
SIMILARITY_THRESHOLD=0.75

# Version mapping threshold
MAPPING_THRESHOLD=0.6

# ===== QA Bot Settings =====
# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.2

# Maximum context length for QA
MAX_CONTEXT_LENGTH=12000

# Top-k results for vector search
QA_TOP_K=30

# ===== Development Settings =====
# Set to true for debug mode
DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO

# ===== Optional: Custom Model Paths =====
# If using local models instead of downloading from HuggingFace
# LOCAL_EMBEDDING_MODEL_PATH=/path/to/local/embedding/model
# LOCAL_LLM_MODEL_PATH=/path/to/local/llm/model